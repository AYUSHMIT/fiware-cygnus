/**
 * Copyright 2015 Telefonica Investigaci√≥n y Desarrollo, S.A.U
 *
 * This file is part of fiware-cygnus (FI-WARE project).
 *
 * fiware-cygnus is free software: you can redistribute it and/or modify it under the terms of the GNU Affero
 * General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your
 * option) any later version.
 * fiware-cygnus is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the
 * implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU Affero General Public License
 * for more details.
 *
 * You should have received a copy of the GNU Affero General Public License along with fiware-cygnus. If not, see
 * http://www.gnu.org/licenses/.
 *
 * For those usages not covered by the GNU Affero General Public License please contact with iot_support at tid dot es
 */
package com.telefonica.iot.cygnus.sinks;

import com.telefonica.iot.cygnus.containers.NotifyContextRequest;
import static com.telefonica.iot.cygnus.sinks.OrionMongoBaseSink.DataModel.COLLECTIONPERSERVICEPATH;
import com.telefonica.iot.cygnus.utils.Utils;
import java.util.ArrayList;
import java.util.Date;
import java.util.Map;
import org.bson.Document;

/**
 * OrionMongoSink will be in charge of persisting Orion context data in a historic fashion within a MongoDB deployment.
 * 
 * The way this sink will build the historics will be very similar to the already existent OrionHDFSSink,
 * OrionMySQLSink and OrionCKANSink, i.e. by appending ("to append" has several means, deppending on the final backend)
 * new raw data to the already existent one.
 * 
 * Because raw data is stored, this sinks differentiates from OrionSTHSink (issue #19), which is in charge of updating
 * already exitent data with new notified data since the goal is to offer aggregated measures to the end user.
 * Nevertheless, in the future most probably the usage of the Mongo Aggregation Framework will allow us to generate
 * such aggregated measures based on the stored raw data; in that case the usage of OrionSTHSink becomes deprecated.
 * 
 * @author frb
 * @author xdelox
 */
public class OrionMongoSink extends OrionMongoBaseSink {

    /**
     * Constructor.
     */
    public OrionMongoSink() {
        super();
    } // OrionMongoSink

    @Override
    void persistOne(Map<String, String> eventHeaders, NotifyContextRequest notification) throws Exception {
        Accumulator accumulator = new Accumulator();
        accumulator.initializeBatching(new Date().getTime());
        accumulator.accumulate(eventHeaders, notification);
        persistBatch(accumulator.getDefaultBatch(), accumulator.getGroupedBatch());
    } // persistOne
/*    
        // get some header values
        Long recvTimeTs = new Long(eventHeaders.get(Constants.HEADER_TIMESTAMP));
        String fiwareService = eventHeaders.get(Constants.HEADER_NOTIFIED_SERVICE);
        String[] servicePaths;
        String[] destinations;
        
        if (enableGrouping) {
            servicePaths = eventHeaders.get(Constants.HEADER_GROUPED_SERVICE_PATHS).split(",");
            destinations = eventHeaders.get(Constants.HEADER_GROUPED_DESTINATIONS).split(",");
        } else {
            servicePaths = eventHeaders.get(Constants.HEADER_DEFAULT_SERVICE_PATHS).split(",");
            destinations = eventHeaders.get(Constants.HEADER_DEFAULT_DESTINATIONS).split(",");
        } // if else
        
        for (int i = 0; i < servicePaths.length; i++) {
            servicePaths[i] = "/" + servicePaths[i]; // this sink uses the removed initial slash
        } // for

        // human readable version of the reception time
        String recvTime = Utils.getHumanReadable(recvTimeTs, true);

        // create the database for this fiwareService if not yet existing... the cost of trying to create it is the same
        // than checking if it exits and then creating it
        String dbName = buildDbName(fiwareService);
        backend.createDatabase(dbName);
        
        // collection name container
        String collectionName = null;

        // create the collection at this stage, if the data model is collection-per-service-path
        if (dataModel == DataModel.COLLECTIONPERSERVICEPATH) {
            for (String fiwareServicePath : servicePaths) {
                collectionName = buildCollectionName(dbName, fiwareServicePath, null, null, false, null, null,
                        fiwareService);
                backend.createCollection(dbName, collectionName);
            } // for
        } // if
        
        // iterate on the contextResponses
        ArrayList contextResponses = notification.getContextResponses();
        
        for (int i = 0; i < contextResponses.size(); i++) {
            NotifyContextRequest.ContextElementResponse contextElementResponse;
            contextElementResponse = (NotifyContextRequest.ContextElementResponse) contextResponses.get(i);
            NotifyContextRequest.ContextElement contextElement = contextElementResponse.getContextElement();
            String entityId = contextElement.getId();
            String entityType = contextElement.getType();
            LOGGER.debug("[" + this.getName() + "] Processing context element (id=" + entityId + ", type= "
                    + entityType + ")");
            
            // create the collection at this stage, if the data model is collection-per-entity
            if (dataModel == DataModel.COLLECTIONPERENTITY) {
                collectionName = buildCollectionName(dbName, servicePaths[i], destinations[i], null, false,
                        entityId, entityType, fiwareService);
                backend.createCollection(dbName, collectionName);
            } // if
            
            // iterate on all this entity's attributes, if there are attributes
            ArrayList<NotifyContextRequest.ContextAttribute> contextAttributes = contextElement.getAttributes();
            
            if (contextAttributes == null || contextAttributes.isEmpty()) {
                LOGGER.warn("No attributes within the notified entity, nothing is done (id=" + entityId
                        + ", type=" + entityType + ")");
                continue;
            } // if

            HashMap attrs = new HashMap();
            HashMap mds = new HashMap();

            for (NotifyContextRequest.ContextAttribute contextAttribute : contextAttributes) {
                String attrName = contextAttribute.getName();
                String attrType = contextAttribute.getType();
                String attrValue = contextAttribute.getContextValue(false);
                String attrMetadata = contextAttribute.getContextMetadata();
                LOGGER.debug("[" + this.getName() + "] Processing context attribute (name=" + attrName + ", type="
                        + attrType + ")");
                
                // create the collection at this stage, if the data model is collection-per-attribute
                if (dataModel == DataModel.COLLECTIONPERATTRIBUTE  && rowAttrPersistence) {
                    collectionName = buildCollectionName(dbName, servicePaths[i], destinations[i], attrName,
                            false, entityId, entityType, fiwareService);
                    backend.createCollection(dbName, collectionName);
                } // if

                if (this.rowAttrPersistence) {
                    LOGGER.info("[" + this.getName() + "] Persisting data at OrionMongoSink. Database: "
                            + dbName + ", Collection: " + collectionName + ", Data: " + recvTimeTs.longValue() / 1000L
                            + "," + recvTime + "," + entityId + "," + entityType + ","
                            + attrName + "," + attrType + "," + attrValue + "," + attrMetadata);
                    this.backend.insertContextDataRaw(
                            dbName, collectionName, recvTimeTs.longValue() / 1000L, recvTime,
                            entityId, entityType, attrName, attrType, attrValue, attrMetadata);
                } else {
                    attrs.put(attrName, attrValue);
                    mds.put(attrName + "_md", attrMetadata);
                }

            } // for
            if (!this.rowAttrPersistence) {
                if (dataModel == DataModel.COLLECTIONPERATTRIBUTE) {
                    LOGGER.warn("Persisting data by columns is useless for collection-per-attribute data model");
                } else {
                    LOGGER.info("[" + this.getName() + "] Persisting data at OrionMongoSink. Database: "
                            + dbName + ", Collection: " + collectionName + ", Data: " + recvTimeTs.longValue() / 1000L
                            + "," + recvTime + "," + entityId + "," + entityType + ","
                            + attrs.toString() + "," + mds.toString() + "]");
                    this.backend.insertContextDataRaw(
                            dbName, collectionName, recvTimeTs.longValue() / 1000L,
                            recvTime, entityId, entityType, attrs, mds);
                }
            }
        } // for
    } // persistOne
*/
    
    @Override
    void persistBatch(Batch defaultBatch, Batch groupedBatch) throws Exception {
        // select batch depending on the enable grouping parameter
        Batch batch = (enableGrouping ? groupedBatch : defaultBatch);
        
        if (batch == null) {
            LOGGER.debug("[" + this.getName() + "] Null batch, nothing to do");
            return;
        } // if
 
        // iterate on the destinations, for each one a single create / append will be performed
        for (String destination : batch.getDestinations()) {
            LOGGER.debug("[" + this.getName() + "] Processing sub-batch regarding the " + destination
                    + " destination");

            // get the sub-batch for this destination
            ArrayList<CygnusEvent> subBatch = batch.getEvents(destination);
            
            // get an aggregator for this destination and initialize it
            MongoAggregator aggregator = getAggregator(rowAttrPersistence);
            aggregator.initialize(subBatch.get(0));

            for (CygnusEvent cygnusEvent : subBatch) {
                aggregator.aggregate(cygnusEvent);
            } // for
            
            // persist the aggregation
            persistAggregation(aggregator);
            batch.setPersisted(destination);
        } // for
    } // persistBatch

    /**
     * Class for aggregating aggregation.
     */
    private abstract class MongoAggregator {
        
        // string containing the data aggregation
        protected ArrayList<Document> aggregation;
        protected String service;
        protected String servicePath;
        protected String destination;
        protected String dbName;
        protected String collectionName;
        
        public MongoAggregator() {
            aggregation = new ArrayList<Document>();
        } // MongoAggregator
        
        public ArrayList<Document> getAggregation() {
            return aggregation;
        } // getAggregation
        
        public String getDbName() {
            return dbName;
        } // getDbName
        
        public String getCollectionName() {
            return collectionName;
        } // getCollectionName
        
        public void initialize(CygnusEvent cygnusEvent) throws Exception {
            service = cygnusEvent.getService();
            servicePath = cygnusEvent.getServicePath();
            destination = cygnusEvent.getDestination();
            dbName = buildDbName(service);
            collectionName = null; // this cannot be built at this moment
        } // initialize
        
        public abstract void aggregate(CygnusEvent cygnusEvent) throws Exception;
        
    } // MongoAggregator
    
    /**
     * Class for aggregating batches in row mode.
     */
    private class RowAggregator extends MongoAggregator {
        
        @Override
        public void aggregate(CygnusEvent cygnusEvent) throws Exception {
            // get the event headers
            long recvTimeTs = cygnusEvent.getRecvTimeTs();
            String recvTime = Utils.getHumanReadable(recvTimeTs, true);

            // get the event body
            NotifyContextRequest.ContextElement contextElement = cygnusEvent.getContextElement();
            String entityId = contextElement.getId();
            String entityType = contextElement.getType();
            LOGGER.debug("[" + getName() + "] Processing context element (id=" + entityId + ", type="
                    + entityType + ")");
            
            // iterate on all this context element attributes, if there are attributes
            ArrayList<NotifyContextRequest.ContextAttribute> contextAttributes = contextElement.getAttributes();

            if (contextAttributes == null || contextAttributes.isEmpty()) {
                LOGGER.warn("No attributes within the notified entity, nothing is done (id=" + entityId
                        + ", type=" + entityType + ")");
                return;
            } // if
            
            for (NotifyContextRequest.ContextAttribute contextAttribute : contextAttributes) {
                String attrName = contextAttribute.getName();
                String attrType = contextAttribute.getType();
                String attrValue = contextAttribute.getContextValue(true);
                String attrMetadata = contextAttribute.getContextMetadata();
                LOGGER.debug("[" + getName() + "] Processing context attribute (name=" + attrName + ", type="
                        + attrType + ")");
                Document doc = new Document("recvTime", new Date(recvTimeTs * 1000));
        
                switch (dataModel) {
                    case COLLECTIONPERSERVICEPATH:
                        doc.append("entityId", entityId)
                                .append("entityType", entityType)
                                .append("attrName", attrName)
                                .append("attrType", attrType)
                                .append("attrValue", attrValue);
                        break;
                    case COLLECTIONPERENTITY:
                        doc.append("attrName", attrName)
                                .append("attrType", attrType)
                                .append("attrValue", attrValue);
                        break;
                    case COLLECTIONPERATTRIBUTE:
                        doc.append("attrType", attrType)
                                .append("attrValue", attrValue);
                        break;
                    default:
                        // this will never be reached
                } // switch

                aggregation.add(doc);
            } // for
        } // aggregate

    } // RowAggregator
    
    /**
     * Class for aggregating batches in column mode.
     */
    private class ColumnAggregator extends MongoAggregator {
        
        @Override
        public void aggregate(CygnusEvent cygnusEvent) throws Exception {
            // get the event headers
            long recvTimeTs = cygnusEvent.getRecvTimeTs();
            String recvTime = Utils.getHumanReadable(recvTimeTs, true);

            // get the event body
            NotifyContextRequest.ContextElement contextElement = cygnusEvent.getContextElement();
            String entityId = contextElement.getId();
            String entityType = contextElement.getType();
            LOGGER.debug("[" + getName() + "] Processing context element (id=" + entityId + ", type="
                    + entityType + ")");
            
            // iterate on all this context element attributes, if there are attributes
            ArrayList<NotifyContextRequest.ContextAttribute> contextAttributes = contextElement.getAttributes();

            if (contextAttributes == null || contextAttributes.isEmpty()) {
                LOGGER.warn("No attributes within the notified entity, nothing is done (id=" + entityId
                        + ", type=" + entityType + ")");
                return;
            } // if
            
            Document doc = new Document("recvTime", new Date(recvTimeTs * 1000));
            
            if (dataModel == COLLECTIONPERSERVICEPATH) {
                doc.append("entityId", entityId)
                        .append("entityType", entityType);
            } // if

            for (NotifyContextRequest.ContextAttribute contextAttribute : contextAttributes) {
                String attrName = contextAttribute.getName();
                String attrType = contextAttribute.getType();
                String attrValue = contextAttribute.getContextValue(true);
                String attrMetadata = contextAttribute.getContextMetadata();
                LOGGER.debug("[" + getName() + "] Processing context attribute (name=" + attrName + ", type="
                        + attrType + ")");
                doc.append(attrName, attrValue);
            } // for

            aggregation.add(doc);
        } // aggregate
        
    } // ColumnAggregator
    
    private MongoAggregator getAggregator(boolean rowAttrPersistence) {
        if (rowAttrPersistence) {
            return new RowAggregator();
        } else {
            return new ColumnAggregator();
        } // if else
    } // getAggregator
    
    private void persistAggregation(MongoAggregator aggregator) throws Exception {
        ArrayList<Document> aggregation = aggregator.getAggregation();
        String dbName = aggregator.getDbName();
        String collectionName = buildCollectionName();
        
        LOGGER.info("[" + this.getName() + "] Persisting data at OrionMongoSink. Database: "
                + dbName + ", Collection: " + collectionName + ", Data: " + aggregation);

        backend.createDatabase(dbName);
        backend.createCollection(dbName, collectionName);
        backend.insertContextDataRaw(dbName, collectionName, aggregation);
    } // persistAggregation
    
} // OrionMongoSink
